{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhfJWjt9Abkm"
   },
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:32.676535Z",
     "iopub.status.busy": "2022-06-01T19:27:32.675839Z",
     "iopub.status.idle": "2022-06-01T19:27:35.067091Z",
     "shell.execute_reply": "2022-06-01T19:27:35.066210Z",
     "shell.execute_reply.started": "2022-06-01T19:27:32.676421Z"
    },
    "id": "MxvuoS-KAbko"
   },
   "outputs": [],
   "source": [
    "# Data Augmentation\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "\n",
    "class DataAugmentation:\n",
    "    def __init__(self,global_crops_scale=(0.4,1),local_crops_scale=(0.05,0.4),n_local_crops=2,output_size=112):\n",
    "\n",
    "        self.n_local_crops=n_local_crops\n",
    "        RandomGaussianBlur=lambda p: transforms.RandomApply([transforms.GaussianBlur(kernel_size=1,sigma=(0.1,2))],p=p)\n",
    "        flip_and_rotation=transforms.Compose([transforms.RandomHorizontalFlip(),transforms.RandomRotation(degrees=(10)),])\n",
    "        normalize=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.1307,),(0.3081,)),])\n",
    "\n",
    "\n",
    "        self.global_1=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(output_size,scale=global_crops_scale,interpolation=InterpolationMode.BICUBIC),\n",
    "            flip_and_rotation,\n",
    "            RandomGaussianBlur(1.0),\n",
    "            normalize\n",
    "        ])\n",
    "        self.global_2=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(output_size,scale=global_crops_scale,interpolation=InterpolationMode.BICUBIC),\n",
    "            flip_and_rotation,\n",
    "            RandomGaussianBlur(0.1),\n",
    "            transforms.RandomSolarize(170,p=0.2),\n",
    "            normalize\n",
    "        ])\n",
    "        self.local=transforms.Compose([\n",
    "            transforms.RandomResizedCrop(output_size,scale=local_crops_scale,interpolation=InterpolationMode.BICUBIC),\n",
    "            flip_and_rotation,\n",
    "            RandomGaussianBlur(0.5),\n",
    "            normalize\n",
    "        ])\n",
    "\n",
    "    \n",
    "    def __call__(self,image):\n",
    "        '''\n",
    "        all_crops:list of torch.Tensor\n",
    "        represent different version of input img\n",
    "        '''\n",
    "        all_crops=[]\n",
    "        all_crops.append(self.global_1(image))\n",
    "        all_crops.append(self.global_2(image))\n",
    "        all_crops.extend([self.local(image) for _ in range(self.n_local_crops)])\n",
    "        return all_crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYomvtkaAbkr"
   },
   "source": [
    "# ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T21:24:59.915973Z",
     "iopub.status.busy": "2022-06-01T21:24:59.915605Z",
     "iopub.status.idle": "2022-06-01T21:24:59.974667Z",
     "shell.execute_reply": "2022-06-01T21:24:59.973741Z",
     "shell.execute_reply.started": "2022-06-01T21:24:59.915941Z"
    },
    "id": "VAv2AavlAbkt"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Mostly copy-paste from timm library.\n",
    "https://github.com/rwightman/pytorch-image-models/blob/master/timm/models/vision_transformer.py\n",
    "\"\"\"\n",
    "from functools import partial\n",
    "from collections import OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def drop_path(x, drop_prob: float = 0., training: bool = False):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    if drop_prob == 0. or not training:\n",
    "        return x\n",
    "    keep_prob = 1 - drop_prob\n",
    "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n",
    "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "    random_tensor.floor_()  # binarize\n",
    "    output = x.div(keep_prob) * random_tensor\n",
    "    return output\n",
    "\n",
    "\n",
    "class DropPath(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop paths (Stochastic Depth) per sample  (when applied in main path of residual blocks).\n",
    "    \"\"\"\n",
    "    def __init__(self, drop_prob=None):\n",
    "        super(DropPath, self).__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "\n",
    "    def forward(self, x):\n",
    "        return drop_path(x, self.drop_prob, self.training)\n",
    "\n",
    "\n",
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"\n",
    "    2D Image to Patch Embedding\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size=224, patch_size=16, in_c=3, embed_dim=768, norm_layer=None):\n",
    "        super().__init__()\n",
    "        img_size = (img_size, img_size)\n",
    "        patch_size = (patch_size, patch_size)\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.grid_size = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "        self.num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.norm = norm_layer(embed_dim) if norm_layer else nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        assert H == self.img_size[0] and W == self.img_size[1], \\\n",
    "            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n",
    "\n",
    "        # flatten: [B, C, H, W] -> [B, C, HW]\n",
    "        # transpose: [B, C, HW] -> [B, HW, C]\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,dim,num_heads=8,qkv_bias=False,qk_scale=None,attn_drop_ratio=0.,proj_drop_ratio=0.):\n",
    "        super(Attention,self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop_ratio)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop_ratio)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [batch_size, num_patches + 1, total_embed_dim]\n",
    "        B, N, C = x.shape\n",
    "\n",
    "        # qkv(): -> [batch_size, num_patches + 1, 3 * total_embed_dim]\n",
    "        # reshape: -> [batch_size, num_patches + 1, 3, num_heads, embed_dim_per_head]\n",
    "        # permute: -> [3, batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        # [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n",
    "\n",
    "        # transpose: -> [batch_size, num_heads, embed_dim_per_head, num_patches + 1]\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, num_patches + 1]\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        # @: multiply -> [batch_size, num_heads, num_patches + 1, embed_dim_per_head]\n",
    "        # transpose: -> [batch_size, num_patches + 1, num_heads, embed_dim_per_head]\n",
    "        # reshape: -> [batch_size, num_patches + 1, total_embed_dim]\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x,attn\n",
    "\n",
    "class Mlp(nn.Module):\n",
    "    \"\"\"\n",
    "    MLP as used in Vision Transformer, MLP-Mixer and related networks\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self,dim,num_heads, mlp_ratio=4.,qkv_bias=False,qk_scale=None, drop_ratio=0.,attn_drop_ratio=0., drop_path_ratio=0.,\n",
    "                 act_layer=nn.GELU,norm_layer=nn.LayerNorm):\n",
    "        super(Block, self).__init__()\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                              attn_drop_ratio=attn_drop_ratio, proj_drop_ratio=drop_ratio)\n",
    "\n",
    "        #  drop path for stochastic depth, we shall see if this is better than dropout here\n",
    "        self.drop_path = DropPath(drop_path_ratio) if drop_path_ratio > 0. else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop_ratio)\n",
    "\n",
    "    def forward(self, x,return_attention=False):\n",
    "        y,attn=self.attn(self.norm1(x))\n",
    "        if return_attention:\n",
    "            return attn\n",
    "        x = x + self.drop_path(y)\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=112, patch_size=7, in_c=3, num_classes=0,\n",
    "                 embed_dim=588, depth=6, num_heads=7, mlp_ratio=4.0, qkv_bias=True,\n",
    "                 qk_scale=None, representation_size=None, distilled=False, drop_ratio=0.,\n",
    "                 attn_drop_ratio=0., drop_path_ratio=0., embed_layer=PatchEmbed, norm_layer=None,\n",
    "                 act_layer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img_size (int, tuple): input image size\n",
    "            patch_size (int, tuple): patch size\n",
    "            in_c (int): number of input channels\n",
    "            num_classes (int): number of classes for classification head\n",
    "            embed_dim (int): embedding dimension\n",
    "            depth (int): depth of transformer\n",
    "            num_heads (int): number of attention heads\n",
    "            mlp_ratio (int): ratio of mlp hidden dim to embedding dim\n",
    "            qkv_bias (bool): enable bias for qkv if True\n",
    "            qk_scale (float): override default qk scale of head_dim ** -0.5 if set\n",
    "            representation_size (Optional[int]): enable and set representation layer (pre-logits) to this value if set\n",
    "            distilled (bool): model includes a distillation token and head as in DeiT models\n",
    "            drop_ratio (float): dropout rate\n",
    "            attn_drop_ratio (float): attention dropout rate\n",
    "            drop_path_ratio (float): stochastic depth rate\n",
    "            embed_layer (nn.Module): patch embedding layer\n",
    "            norm_layer: (nn.Module): normalization layer\n",
    "        \"\"\"\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models\n",
    "        self.num_tokens = 2 if distilled else 1\n",
    "        norm_layer = norm_layer or partial(nn.LayerNorm, eps=1e-6)\n",
    "        act_layer = act_layer or nn.GELU\n",
    "\n",
    "        self.patch_embed = embed_layer(img_size=img_size, patch_size=patch_size, in_c=in_c, embed_dim=embed_dim)\n",
    "        num_patches = self.patch_embed.num_patches\n",
    "\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.dist_token = nn.Parameter(torch.zeros(1, 1, embed_dim)) if distilled else None\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + self.num_tokens, embed_dim))\n",
    "        self.pos_drop = nn.Dropout(p=drop_ratio)\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_ratio, depth)]  # stochastic depth decay rule\n",
    "        self.blocks = nn.Sequential(*[\n",
    "            Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,\n",
    "                  drop_ratio=drop_ratio, attn_drop_ratio=attn_drop_ratio, drop_path_ratio=dpr[i],\n",
    "                  norm_layer=norm_layer, act_layer=act_layer)\n",
    "            for i in range(depth)\n",
    "        ])\n",
    "        self.norm = norm_layer(embed_dim)\n",
    "\n",
    "        # Representation layer\n",
    "        if representation_size and not distilled:\n",
    "            self.has_logits = True\n",
    "            self.num_features = representation_size\n",
    "            self.pre_logits = nn.Sequential(OrderedDict([\n",
    "                (\"fc\", nn.Linear(embed_dim, representation_size)),\n",
    "                (\"act\", nn.Tanh())\n",
    "            ]))\n",
    "        else:\n",
    "            self.has_logits = False\n",
    "            self.pre_logits = nn.Identity()\n",
    "\n",
    "        # Classifier head(s)\n",
    "        self.head = nn.Linear(self.num_features, num_classes) if num_classes > 0 else nn.Identity()\n",
    "        self.head_dist = None\n",
    "        if distilled:\n",
    "            self.head_dist = nn.Linear(self.embed_dim, self.num_classes) if num_classes > 0 else nn.Identity()\n",
    "\n",
    "        # Weight init\n",
    "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
    "        if self.dist_token is not None:\n",
    "            nn.init.trunc_normal_(self.dist_token, std=0.02)\n",
    "\n",
    "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
    "        self.apply(self._init_vit_weights)\n",
    "    \n",
    "    \n",
    "    def _init_vit_weights(self,m):\n",
    "        \"\"\"\n",
    "        ViT weight initialization\n",
    "        :param m: module\n",
    "        \"\"\"\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.01)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, mode=\"fan_out\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.zeros_(m.bias)\n",
    "            nn.init.ones_(m.weight)\n",
    "\n",
    "#     def forward_features(self, x):\n",
    "#         # [B, C, H, W] -> [B, num_patches, embed_dim]\n",
    "#         x = self.patch_embed(x)  # [B, 196, 768]\n",
    "#         # [1, 1, 768] -> [B, 1, 768]\n",
    "#         cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "#         if self.dist_token is None:\n",
    "#             x = torch.cat((cls_token, x), dim=1)  # [B, 197, 768]\n",
    "#         else:\n",
    "#             x = torch.cat((cls_token, self.dist_token.expand(x.shape[0], -1, -1), x), dim=1)\n",
    "\n",
    "#         x = self.pos_drop(x + self.pos_embed)\n",
    "#         x = self.blocks(x)\n",
    "#         x = self.norm(x)\n",
    "#         if self.dist_token is None:\n",
    "#             return self.pre_logits(x[:, 0])\n",
    "#         else:\n",
    "#             return x[:, 0], x[:, 1]\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.forward_features(x)\n",
    "#         if self.head_dist is not None:\n",
    "#             x, x_dist = self.head(x[0]), self.head_dist(x[1])\n",
    "#             if self.training and not torch.jit.is_scripting():\n",
    "#                 # during inference, return the average of both classifier predictions\n",
    "#                 return x, x_dist\n",
    "#             else:\n",
    "#                 return (x + x_dist) / 2\n",
    "#         else:\n",
    "#             x = self.head(x)\n",
    "#         return x\n",
    "    def interpolate_pos_encoding(self, x, w, h):\n",
    "        npatch = x.shape[1] - 1\n",
    "        N = self.pos_embed.shape[1] - 1\n",
    "        if npatch == N and w == h:\n",
    "            return self.pos_embed\n",
    "        class_pos_embed = self.pos_embed[:, 0]\n",
    "        patch_pos_embed = self.pos_embed[:, 1:]\n",
    "        dim = x.shape[-1]\n",
    "        w0 = w // self.patch_embed.patch_size\n",
    "        h0 = h // self.patch_embed.patch_size\n",
    "        # we add a small number to avoid floating point error in the interpolation\n",
    "        # see discussion at https://github.com/facebookresearch/dino/issues/8\n",
    "        w0, h0 = w0 + 0.1, h0 + 0.1\n",
    "        patch_pos_embed = nn.functional.interpolate(\n",
    "            patch_pos_embed.reshape(1, int(math.sqrt(N)), int(math.sqrt(N)), dim).permute(0, 3, 1, 2),\n",
    "            scale_factor=(w0 / math.sqrt(N), h0 / math.sqrt(N)),\n",
    "            mode='bicubic',\n",
    "        )\n",
    "        assert int(w0) == patch_pos_embed.shape[-2] and int(h0) == patch_pos_embed.shape[-1]\n",
    "        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n",
    "        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n",
    "\n",
    "    def prepare_tokens(self, x):\n",
    "        B, nc, w, h = x.shape\n",
    "        x = self.patch_embed(x)  # patch linear embedding\n",
    "\n",
    "        # add the [CLS] token to the embed patch tokens\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "\n",
    "        # add positional encoding to each token\n",
    "        x = x + self.interpolate_pos_encoding(x, w, h)\n",
    "\n",
    "        return self.pos_drop(x)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        x = self.norm(x)\n",
    "        return x[:, 0]\n",
    "\n",
    "    def get_last_selfattention(self, x):\n",
    "        x = self.prepare_tokens(x)\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            if i < len(self.blocks) - 1:\n",
    "                x = blk(x)\n",
    "            else:\n",
    "                # return attention of the last block\n",
    "                return blk(x, return_attention=True)\n",
    "            \n",
    "    def get_intermediate_layers(self, x, n=1):\n",
    "        x = self.prepare_tokens(x)\n",
    "        # we return the output tokens from the `n` last blocks\n",
    "        output = []\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x = blk(x)\n",
    "            if len(self.blocks) - i <= n:\n",
    "                output.append(self.norm(x))\n",
    "        return output\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t75_iQwmAbkv"
   },
   "source": [
    "# new Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.160336Z",
     "iopub.status.busy": "2022-06-01T19:27:35.157832Z",
     "iopub.status.idle": "2022-06-01T19:27:35.179816Z",
     "shell.execute_reply": "2022-06-01T19:27:35.179079Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.160300Z"
    },
    "id": "qEYefO_RAbkw"
   },
   "outputs": [],
   "source": [
    "class DINOHead(nn.Module):\n",
    "    \"\"\"Network hooked up to the CLS token embedding.\n",
    "    Just a MLP with the last layer being normalized in a particular way.\n",
    "    \n",
    "    Parameters:\n",
    "    in_dim : int\n",
    "        The dimensionality of the token embedding.\n",
    "    out_dim : int\n",
    "        The dimensionality of the final layer (we compute the softmax over).\n",
    "    hidden_dim : int\n",
    "        Dimensionality of the hidden layers.\n",
    "    bottleneck_dim : int\n",
    "        Dimensionality of the second last layer.\n",
    "    n_layers : int\n",
    "        The number of layers.\n",
    "    norm_last_layer : bool\n",
    "        If True, then we freeze the norm of the weight of the last linear layer\n",
    "        to 1.\n",
    "        \n",
    "        \n",
    "    Attributes:\n",
    "    mlp : nn.Sequential\n",
    "        Vanilla multi-layer perceptron.\n",
    "    last_layer : nn.Linear\n",
    "        Reparametrized linear layer with weight normalization. That means\n",
    "        that that it will have `weight_g` and `weight_v` as learnable\n",
    "        parameters instead of a single `weight`.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, use_bn=False, norm_last_layer=True, nlayers=3, hidden_dim=2048, bottleneck_dim=256):\n",
    "        super().__init__()\n",
    "        nlayers = max(nlayers, 1)\n",
    "        if nlayers == 1:\n",
    "            self.mlp = nn.Linear(in_dim, bottleneck_dim)\n",
    "        else:\n",
    "            layers = [nn.Linear(in_dim, hidden_dim)]\n",
    "            if use_bn:\n",
    "                layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.GELU())\n",
    "            for _ in range(nlayers - 2):\n",
    "                layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "                if use_bn:\n",
    "                    layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "                layers.append(nn.GELU())\n",
    "            layers.append(nn.Linear(hidden_dim, bottleneck_dim))\n",
    "            self.mlp = nn.Sequential(*layers)\n",
    "        self.apply(self._init_weights)\n",
    "        self.last_layer = nn.utils.weight_norm(nn.Linear(bottleneck_dim, out_dim, bias=False))\n",
    "        self.last_layer.weight_g.data.fill_(1)\n",
    "        if norm_last_layer:\n",
    "            self.last_layer.weight_g.requires_grad = False\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Run forward pass.\n",
    "        \n",
    "        Parameters:\n",
    "        x : torch.Tensor\n",
    "            Of shape `(n_samples, in_dim)`.\n",
    "        \n",
    "        return: torch.Tensor\n",
    "            Of shape `(n_samples, out_dim)`.\n",
    "        \"\"\"\n",
    "        x = self.mlp(x)\n",
    "        x = nn.functional.normalize(x, dim=-1, p=2)\n",
    "        x = self.last_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QdCivSGMAbkw"
   },
   "source": [
    "# Multicropwrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.188015Z",
     "iopub.status.busy": "2022-06-01T19:27:35.185582Z",
     "iopub.status.idle": "2022-06-01T19:27:35.199837Z",
     "shell.execute_reply": "2022-06-01T19:27:35.198951Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.187978Z"
    },
    "id": "h8M8Hs8pAbkx"
   },
   "outputs": [],
   "source": [
    "class MultiCropWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    Perform forward pass separately on each resolution input.\n",
    "    The inputs corresponding to a single resolution are clubbed and single\n",
    "    forward is run on the same resolution inputs. Hence we do several\n",
    "    forward passes = number of different resolutions used. We then\n",
    "    concatenate all the output features and run the head forward on these\n",
    "    concatenated features.\n",
    "\n",
    "    Parameters:\n",
    "    backbone : vision transformer\n",
    "        Instantiated Vision Transformer. Note that we will take the `head` attribute and replace it with `nn.Identity`.\n",
    "    head : DINOHead\n",
    "        New head that is going to be put on top of the `backbone`.\n",
    "    \"\"\"\n",
    "    def __init__(self, backbone, head):\n",
    "        super(MultiCropWrapper, self).__init__()\n",
    "        # disable layers dedicated to ImageNet labels classification\n",
    "        backbone.fc, backbone.head = nn.Identity(), nn.Identity()\n",
    "        self.backbone = backbone\n",
    "        self.head = head\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        The different crops are concatenated along the batch dimension and then a single forward pass is fun. The resulting tensor\n",
    "        is then chunked back to per crop tensors.\n",
    "        return: list of len=n_crops, each of shape (batch,out_dim)\n",
    "        '''\n",
    "        # convert to list\n",
    "        if not isinstance(x, list):\n",
    "            print('multicrop',x.shape)\n",
    "            x = [x]\n",
    "        n_crops=len(x)\n",
    "        concatenated=torch.cat(x,dim=0)\n",
    "        cls_embedding=self.backbone(concatenated)\n",
    "        logits=self.head(cls_embedding)\n",
    "        chunks=logits.chunk(n_crops)\n",
    "        return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z-WeOIWAbky"
   },
   "source": [
    "# simCLR loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.206562Z",
     "iopub.status.busy": "2022-06-01T19:27:35.204048Z",
     "iopub.status.idle": "2022-06-01T19:27:35.224697Z",
     "shell.execute_reply": "2022-06-01T19:27:35.223981Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.206525Z"
    },
    "id": "I_QXtsplAbkz"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Loss(nn.Module):\n",
    "    def __init__(self, tau=0.07,out_dim=1024,center_momentum=0.995):\n",
    "        super().__init__()\n",
    "        self.tau=tau\n",
    "        self.center_momentum=center_momentum\n",
    "        self.register_buffer(\"center\", torch.zeros(1, out_dim))\n",
    "\n",
    "    def forward(self, student_output_ori, teacher_output_ori):\n",
    "        \"\"\"\n",
    "        NCELoss of the teacher and student networks.\n",
    "        student_output_ori: list of len=n_crops, each of shape (batch,out_dim)\n",
    "        \"\"\"\n",
    "        teacher_output=torch.cat(teacher_output_ori,dim=1)\n",
    "        student_output=torch.cat(student_output_ori,dim=1)\n",
    "        n_examples,_=student_output.size()\n",
    "        teacher=F.normalize(teacher_output,dim=-1)\n",
    "        student=F.normalize(student_output,dim=-1)\n",
    "        scores=torch.mm(teacher,student.t()).div_(self.tau)\n",
    "        target=torch.arange(n_examples,dtype=torch.long).to(scores.device)\n",
    "        loss=F.cross_entropy(scores,target)\n",
    "        self.update_center(teacher_output_ori)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update_center(self, teacher_output):\n",
    "        \"\"\"Update center used for teacher output.\n",
    "        Compute the exponential moving average.\n",
    "        Parameters\n",
    "        ----------\n",
    "        teacher_output : tuple\n",
    "            Tuple of tensors of shape `(n_samples, out_dim)` where each\n",
    "            tensor represents a different crop.\n",
    "        \"\"\"\n",
    "        batch_center = torch.cat(teacher_output).mean(\n",
    "            dim=0, keepdim=True\n",
    "        )  # (1, out_dim)\n",
    "        self.center = self.center * self.center_momentum + batch_center * (\n",
    "            1 - self.center_momentum\n",
    "        )\n",
    "\n",
    "    \n",
    "def clip_gradients(model, clip=2.0):\n",
    "    \"\"\"Rescale norm of computed gradients. Used to avoid gradient exponential\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        Module.\n",
    "    clip : float\n",
    "        Maximum norm.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        if p.grad is not None:\n",
    "            param_norm = p.grad.data.norm(2)\n",
    "            clip_coef = clip / (param_norm + 1e-6)\n",
    "            if clip_coef < 1:\n",
    "                p.grad.data.mul_(clip_coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.231228Z",
     "iopub.status.busy": "2022-06-01T19:27:35.228896Z",
     "iopub.status.idle": "2022-06-01T19:27:35.240060Z",
     "shell.execute_reply": "2022-06-01T19:27:35.239334Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.231191Z"
    },
    "id": "uIT57jeuAbk0"
   },
   "outputs": [],
   "source": [
    "def get_params_groups(model):\n",
    "    regularized = []\n",
    "    not_regularized = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        # we do not regularize biases nor Norm parameters\n",
    "        if name.endswith(\".bias\") or len(param.shape) == 1:\n",
    "            not_regularized.append(param)\n",
    "        else:\n",
    "            regularized.append(param)\n",
    "    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetricLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.246942Z",
     "iopub.status.busy": "2022-06-01T19:27:35.244425Z",
     "iopub.status.idle": "2022-06-01T19:27:35.288296Z",
     "shell.execute_reply": "2022-06-01T19:27:35.287410Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.246889Z"
    },
    "id": "v2zOFUJ7Abk1"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict,deque\n",
    "import datetime\n",
    "import time\n",
    "class SmoothedValue(object):\n",
    "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
    "    window or the global series average.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, window_size=20, fmt=None):\n",
    "        if fmt is None:\n",
    "            fmt = \"{median:.6f} ({global_avg:.6f})\"\n",
    "        self.deque = deque(maxlen=window_size)\n",
    "        self.total = 0.0\n",
    "        self.count = 0\n",
    "        self.fmt = fmt\n",
    "\n",
    "    def update(self, value, n=1):\n",
    "        self.deque.append(value)\n",
    "        self.count += n\n",
    "        self.total += value * n\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        \"\"\"\n",
    "        Warning: does not synchronize the deque!\n",
    "        \"\"\"\n",
    "        if not is_dist_avail_and_initialized():\n",
    "            return\n",
    "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device='cuda')\n",
    "        dist.barrier()\n",
    "        dist.all_reduce(t)\n",
    "        t = t.tolist()\n",
    "        self.count = int(t[0])\n",
    "        self.total = t[1]\n",
    "\n",
    "    @property\n",
    "    def median(self):\n",
    "        d = torch.tensor(list(self.deque))\n",
    "        return d.median().item()\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
    "        return d.mean().item()\n",
    "\n",
    "    @property\n",
    "    def global_avg(self):\n",
    "        return self.total / self.count\n",
    "\n",
    "    @property\n",
    "    def max(self):\n",
    "        return max(self.deque)\n",
    "\n",
    "    @property\n",
    "    def value(self):\n",
    "        return self.deque[-1]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.fmt.format(\n",
    "            median=self.median,\n",
    "            avg=self.avg,\n",
    "            global_avg=self.global_avg,\n",
    "            max=self.max,\n",
    "            value=self.value)\n",
    "\n",
    "class MetricLogger(object):\n",
    "    def __init__(self, delimiter=\"\\t\"):\n",
    "        self.meters = defaultdict(SmoothedValue)\n",
    "        self.delimiter = delimiter\n",
    "\n",
    "    def update(self, **kwargs):\n",
    "        for k, v in kwargs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                v = v.item()\n",
    "            assert isinstance(v, (float, int))\n",
    "            self.meters[k].update(v)\n",
    "\n",
    "    def __getattr__(self, attr):\n",
    "        if attr in self.meters:\n",
    "            return self.meters[attr]\n",
    "        if attr in self.__dict__:\n",
    "            return self.__dict__[attr]\n",
    "        raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n",
    "            type(self).__name__, attr))\n",
    "\n",
    "    def __str__(self):\n",
    "        loss_str = []\n",
    "        for name, meter in self.meters.items():\n",
    "            loss_str.append(\n",
    "                \"{}: {}\".format(name, str(meter))\n",
    "            )\n",
    "        return self.delimiter.join(loss_str)\n",
    "\n",
    "    def synchronize_between_processes(self):\n",
    "        for meter in self.meters.values():\n",
    "            meter.synchronize_between_processes()\n",
    "\n",
    "    def add_meter(self, name, meter):\n",
    "        self.meters[name] = meter\n",
    "\n",
    "    def log_every(self, iterable, print_freq, header=None):\n",
    "        i = 0\n",
    "        if not header:\n",
    "            header = ''\n",
    "        start_time = time.time()\n",
    "        end = time.time()\n",
    "        iter_time = SmoothedValue(fmt='{avg:.6f}')\n",
    "        data_time = SmoothedValue(fmt='{avg:.6f}')\n",
    "        space_fmt = ':' + str(len(str(len(iterable)))) + 'd'\n",
    "        if torch.cuda.is_available():\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}',\n",
    "                'max mem: {memory:.0f}'\n",
    "            ])\n",
    "        else:\n",
    "            log_msg = self.delimiter.join([\n",
    "                header,\n",
    "                '[{0' + space_fmt + '}/{1}]',\n",
    "                'eta: {eta}',\n",
    "                '{meters}',\n",
    "                'time: {time}',\n",
    "                'data: {data}'\n",
    "            ])\n",
    "        MB = 1024.0 * 1024.0\n",
    "        for obj in iterable:\n",
    "            data_time.update(time.time() - end)\n",
    "            yield obj\n",
    "            iter_time.update(time.time() - end)\n",
    "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
    "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
    "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
    "                if torch.cuda.is_available():\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time),\n",
    "                        memory=torch.cuda.max_memory_allocated() / MB))\n",
    "                else:\n",
    "                    print(log_msg.format(\n",
    "                        i, len(iterable), eta=eta_string,\n",
    "                        meters=str(self),\n",
    "                        time=str(iter_time), data=str(data_time)))\n",
    "            i += 1\n",
    "            end = time.time()\n",
    "        total_time = time.time() - start_time\n",
    "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "        print('{} Total time: {} ({:.6f} s / it)'.format(\n",
    "            header, total_time_str, total_time / len(iterable)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.291142Z",
     "iopub.status.busy": "2022-06-01T19:27:35.289468Z",
     "iopub.status.idle": "2022-06-01T19:27:35.304847Z",
     "shell.execute_reply": "2022-06-01T19:27:35.303878Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.291105Z"
    },
    "id": "kGSMHx1rAbk2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "\n",
    "def get_params_groups(model):\n",
    "    regularized = []\n",
    "    not_regularized = []\n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        # we do not regularize biases nor Norm parameters\n",
    "        if name.endswith(\".bias\") or len(param.shape) == 1:\n",
    "            not_regularized.append(param)\n",
    "        else:\n",
    "            regularized.append(param)\n",
    "    return [{'params': regularized}, {'params': not_regularized, 'weight_decay': 0.}]\n",
    "\n",
    "def cosine_scheduler(base_value, final_value, epochs, niter_per_ep, warmup_epochs=0, start_warmup_value=0):\n",
    "    warmup_schedule = np.array([])\n",
    "    warmup_iters = warmup_epochs * niter_per_ep\n",
    "    if warmup_epochs > 0:\n",
    "        warmup_schedule = np.linspace(start_warmup_value, base_value, warmup_iters)\n",
    "\n",
    "    iters = np.arange(epochs * niter_per_ep - warmup_iters)\n",
    "    schedule = final_value + 0.5 * (base_value - final_value) * (1 + np.cos(np.pi * iters / len(iters)))\n",
    "\n",
    "    schedule = np.concatenate((warmup_schedule, schedule))\n",
    "    assert len(schedule) == epochs * niter_per_ep\n",
    "    return schedule\n",
    "\n",
    "def is_dist_avail_and_initialized():\n",
    "    if not dist.is_available():\n",
    "        return False\n",
    "    if not dist.is_initialized():\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def get_world_size():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 1\n",
    "    return dist.get_world_size()\n",
    "\n",
    "def get_rank():\n",
    "    if not is_dist_avail_and_initialized():\n",
    "        return 0\n",
    "    return dist.get_rank()\n",
    "    \n",
    "def is_main_process():\n",
    "    return get_rank() == 0\n",
    "\n",
    "def save_on_master(*args, **kwargs):\n",
    "    if is_main_process():\n",
    "        torch.save(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7iBNUPmbAbk2"
   },
   "source": [
    "# training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:27:35.307082Z",
     "iopub.status.busy": "2022-06-01T19:27:35.306371Z",
     "iopub.status.idle": "2022-06-01T19:27:35.317469Z",
     "shell.execute_reply": "2022-06-01T19:27:35.316304Z",
     "shell.execute_reply.started": "2022-06-01T19:27:35.307045Z"
    },
    "id": "JDIILn1aAbk3"
   },
   "outputs": [],
   "source": [
    "def cancel_gradients_last_layer(epoch, model, freeze_last_layer):\n",
    "    if epoch >= freeze_last_layer:\n",
    "        return\n",
    "    for n, p in model.named_parameters():\n",
    "        if \"last_layer\" in n:\n",
    "            p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:47:08.503717Z",
     "iopub.status.busy": "2022-06-01T19:47:08.503359Z",
     "iopub.status.idle": "2022-06-01T19:47:08.518605Z",
     "shell.execute_reply": "2022-06-01T19:47:08.517802Z",
     "shell.execute_reply.started": "2022-06-01T19:47:08.503686Z"
    },
    "id": "_afriAkWAbk3"
   },
   "outputs": [],
   "source": [
    "def train_one_epoch(student,teacher,dino_loss,data_loader,optimizer,lr_schedule,wd_schedule,momentum_schedule,epoch,output_dir\n",
    "                    ,total_epochs,clip_grad,freeze_last_layer):\n",
    "    metric_logger = MetricLogger(delimiter=\"  \")\n",
    "    header = 'Epoch: [{}/{}]'.format(epoch, total_epochs)\n",
    "    for it, (images, _) in enumerate(metric_logger.log_every(data_loader, 1000, header)):\n",
    "        # update weight decay and learning rate according to their schedule\n",
    "        it = len(data_loader) * epoch + it  # global training iteration\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            param_group[\"lr\"] = lr_schedule[it]\n",
    "            if i == 0:  # only the first group is regularized\n",
    "                param_group[\"weight_decay\"] = wd_schedule[it]\n",
    "\n",
    "        # move images to gpu\n",
    "        images = [im.cuda() for im in images]\n",
    "        teacher_output=teacher(images)\n",
    "        student_output=student(images)\n",
    "        loss=dino_loss(student_output,teacher_output)\n",
    "        #print('loss:{:.4f}, stopping training'.format(loss.item()))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        param_norms=clip_gradients(student,clip=clip_grad)\n",
    "        cancel_gradients_last_layer(epoch,student,freeze_last_layer)\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        #EMA update teacher\n",
    "        with torch.no_grad():\n",
    "            m = momentum_schedule[it]  # momentum parameter\n",
    "            for param_q, param_k in zip(student.parameters(), teacher.parameters()):\n",
    "                param_k.data.mul_(m).add_((1 - m) * param_q.detach().data)\n",
    "\n",
    "        #torch.cuda.synchronize()\n",
    "        metric_logger.update(loss=loss.item())\n",
    "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
    "        metric_logger.update(wd=optimizer.param_groups[0][\"weight_decay\"])\n",
    "\n",
    "    # gather the stats from all processes\n",
    "    metric_logger.synchronize_between_processes()\n",
    "    print(\"Averaged stats:\", metric_logger)\n",
    "    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4azdn0DQixp6"
   },
   "outputs": [],
   "source": [
    "def restart_from_checkpoint(ckp_path, run_variables=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Re-start from checkpoint\n",
    "    \"\"\"\n",
    "    if not os.path.isfile(ckp_path):\n",
    "        return\n",
    "    print(\"Found checkpoint at {}\".format(ckp_path))\n",
    "\n",
    "    # open checkpoint file\n",
    "    checkpoint = torch.load(ckp_path, map_location=\"cpu\")\n",
    "\n",
    "    # key is what to look for in the checkpoint file\n",
    "    # value is the object to load\n",
    "    # example: {'state_dict': model}\n",
    "    for key, value in kwargs.items():\n",
    "        if key in checkpoint and value is not None:\n",
    "            try:\n",
    "                msg = value.load_state_dict(checkpoint[key], strict=False)\n",
    "                print(\"=> loaded '{}' from checkpoint '{}' with msg {}\".format(key, ckp_path, msg))\n",
    "            except TypeError:\n",
    "                try:\n",
    "                    msg = value.load_state_dict(checkpoint[key])\n",
    "                    print(\"=> loaded '{}' from checkpoint: '{}'\".format(key, ckp_path))\n",
    "                except ValueError:\n",
    "                    print(\"=> failed to load '{}' from checkpoint: '{}'\".format(key, ckp_path))\n",
    "        else:\n",
    "            print(\"=> key '{}' not found in checkpoint: '{}'\".format(key, ckp_path))\n",
    "\n",
    "    # re load variable important for the run\n",
    "    if run_variables is not None:\n",
    "        for var_name in run_variables:\n",
    "            if var_name in checkpoint:\n",
    "                run_variables[var_name] = checkpoint[var_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-01T19:48:58.745289Z",
     "iopub.status.busy": "2022-06-01T19:48:58.744849Z",
     "iopub.status.idle": "2022-06-01T19:48:58.903496Z",
     "shell.execute_reply": "2022-06-01T19:48:58.902762Z",
     "shell.execute_reply.started": "2022-06-01T19:48:58.745257Z"
    },
    "id": "vwsOTHmbAbk4"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DistributedSampler,DataLoader\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "def train_dino(data_path,batch_size,lr,weight_decay,weight_decay_end,min_lr,out_dim,tau,total_epochs,warmup_epochs,momentum_teacher,output_dir,saveckp_freq,\n",
    "            clip_grad,freeze_last_layer):\n",
    "\n",
    "    transform = DataAugmentation()\n",
    "    dataset = ImageFolder(data_path, transform=transform)\n",
    "    data_loader = DataLoader(dataset,batch_size=batch_size,drop_last=True,)\n",
    "    print(f\"Data loaded: there are {len(dataset)} images.\")\n",
    "\n",
    "    student =VisionTransformer(patch_size=7,drop_path_ratio=0.1,)  # stochastic depth\n",
    "    teacher = VisionTransformer(patch_size=7)\n",
    "    embed_dim = student.embed_dim\n",
    "\n",
    "    student = MultiCropWrapper(student, DINOHead(embed_dim,out_dim=out_dim,use_bn=False,norm_last_layer=True,))\n",
    "    teacher = MultiCropWrapper(teacher,DINOHead(embed_dim, out_dim=out_dim, use_bn=False),)\n",
    "    # move networks to gpu\n",
    "    student=student.cuda()\n",
    "    teacher=teacher.cuda()\n",
    "    params_groups = get_params_groups(student)\n",
    "    optimizer = torch.optim.AdamW(params_groups)\n",
    "    # teacher and student start with the same weights\n",
    "    teacher.load_state_dict(student.state_dict())\n",
    "    dino_loss = Loss(tau=tau,out_dim=out_dim).cuda()\n",
    "    #there is no backpropagation through the teacher, so no need for gradients\n",
    "    for p in teacher.parameters():\n",
    "        p.requires_grad = False\n",
    "    print(f\"Student and Teacher are built: they are both vit network.\")\n",
    "\n",
    "\n",
    "    lr_schedule = cosine_scheduler(lr * (batch_size* get_world_size()) / 256.,min_lr, total_epochs, len(data_loader),warmup_epochs=warmup_epochs,)\n",
    "    wd_schedule = cosine_scheduler(weight_decay,weight_decay_end,total_epochs, len(data_loader),)\n",
    "    # momentum parameter is increased to 1. during training with a cosine schedule\n",
    "    momentum_schedule = cosine_scheduler(momentum_teacher, 1,total_epochs, len(data_loader))\n",
    "    print(f\"Loss, optimizer and schedulers ready.\")\n",
    "\n",
    "    #start to training\n",
    "    to_restore = {\"epoch\": 21}\n",
    "    restart_from_checkpoint(\n",
    "        os.path.join(output_dir, \"checkpoint20.pth\"),\n",
    "        run_variables=to_restore,\n",
    "        student=student,\n",
    "        teacher=teacher,\n",
    "        optimizer=optimizer,\n",
    "        dino_loss=dino_loss,\n",
    "    )\n",
    "    \n",
    "    start_epoch = to_restore[\"epoch\"]\n",
    "    start_time = time.time()\n",
    "    print(\"Starting DINO training !\")\n",
    "    \n",
    "    for epoch in range(start_epoch,total_epochs):\n",
    "        train_stats=train_one_epoch(student,teacher,dino_loss,data_loader,optimizer,lr_schedule,wd_schedule,momentum_schedule,epoch,output_dir,\n",
    "                                    total_epochs,clip_grad,freeze_last_layer)\n",
    "        \n",
    "        save_dict = {'student': student.state_dict(),'teacher': teacher.state_dict(),'optimizer': optimizer.state_dict(),'epoch': epoch + 1,'loss': dino_loss.state_dict()}\n",
    "        if epoch%saveckp_freq==0:\n",
    "            save_on_master(save_dict,os.path.join(output_dir,'checkpoint{}.pth'.format(epoch)))\n",
    "        \n",
    "        log_stats={**{f'train_{k}': v for k, v in train_stats.items()},'epoch': epoch}\n",
    "        with (Path(output_dir)/'log.txt').open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats)+'\\n')\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "    print('Training time {}'.format(total_time_str))\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "execution": {
     "iopub.execute_input": "2022-06-01T19:49:03.128724Z",
     "iopub.status.busy": "2022-06-01T19:49:03.128351Z",
     "iopub.status.idle": "2022-06-01T21:09:35.675095Z",
     "shell.execute_reply": "2022-06-01T21:09:35.673622Z",
     "shell.execute_reply.started": "2022-06-01T19:49:03.128695Z"
    },
    "executionInfo": {
     "elapsed": 26274622,
     "status": "error",
     "timestamp": 1654222071601,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "K6eTDhg8Abk5",
    "outputId": "3e60c981-cb8e-484e-b7f3-92d0647de369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: there are 60000 images.\n",
      "Student and Teacher are built: they are both vit network.\n",
      "Loss, optimizer and schedulers ready.\n",
      "Found checkpoint at /content/drive/MyDrive/MNIST/log/checkpoint10.pth\n",
      "=> loaded 'student' from checkpoint '/content/drive/MyDrive/MNIST/log/checkpoint10.pth' with msg <All keys matched successfully>\n",
      "=> loaded 'teacher' from checkpoint '/content/drive/MyDrive/MNIST/log/checkpoint10.pth' with msg <All keys matched successfully>\n",
      "=> loaded 'optimizer' from checkpoint: '/content/drive/MyDrive/MNIST/log/checkpoint10.pth'\n",
      "=> key 'dino_loss' not found in checkpoint: '/content/drive/MyDrive/MNIST/log/checkpoint10.pth'\n",
      "Starting DINO training !\n",
      "Epoch: [11/100]  [   0/1875]  eta: 0:44:27  loss: 0.002687 (0.002687)  lr: 0.000062 (0.000062)  wd: 0.050641 (0.050641)  time: 1.422853  data: 0.187907  max mem: 10421\n",
      "Epoch: [11/100]  [1000/1875]  eta: 0:20:23  loss: 0.002478 (0.002676)  lr: 0.000062 (0.000062)  wd: 0.051676 (0.051160)  time: 1.403273  data: 0.172565  max mem: 10519\n",
      "Epoch: [11/100]  [1874/1875]  eta: 0:00:01  loss: 0.002043 (0.002474)  lr: 0.000062 (0.000062)  wd: 0.052628 (0.051626)  time: 1.401595  data: 0.170891  max mem: 10519\n",
      "Epoch: [11/100] Total time: 0:43:42 (1.398445 s / it)\n",
      "Averaged stats: loss: 0.002043 (0.002474)  lr: 0.000062 (0.000062)  wd: 0.052628 (0.051626)\n",
      "Epoch: [12/100]  [   0/1875]  eta: 0:43:42  loss: 0.002533 (0.002533)  lr: 0.000062 (0.000062)  wd: 0.052640 (0.052640)  time: 1.398595  data: 0.168281  max mem: 10519\n",
      "Epoch: [12/100]  [1000/1875]  eta: 0:20:24  loss: 0.002681 (0.008382)  lr: 0.000062 (0.000062)  wd: 0.053762 (0.053203)  time: 1.405820  data: 0.174129  max mem: 10519\n",
      "Epoch: [12/100]  [1874/1875]  eta: 0:00:01  loss: 0.002416 (0.005635)  lr: 0.000062 (0.000062)  wd: 0.054791 (0.053708)  time: 1.397452  data: 0.167219  max mem: 10519\n",
      "Epoch: [12/100] Total time: 0:43:40 (1.397539 s / it)\n",
      "Averaged stats: loss: 0.002416 (0.005635)  lr: 0.000062 (0.000062)  wd: 0.054791 (0.053708)\n",
      "Epoch: [13/100]  [   0/1875]  eta: 0:43:43  loss: 0.002765 (0.002765)  lr: 0.000062 (0.000062)  wd: 0.054804 (0.054804)  time: 1.399336  data: 0.168803  max mem: 10519\n",
      "Epoch: [13/100]  [1000/1875]  eta: 0:20:22  loss: 0.002429 (0.002713)  lr: 0.000062 (0.000062)  wd: 0.056013 (0.055411)  time: 1.400642  data: 0.169537  max mem: 10519\n",
      "Epoch: [13/100]  [1874/1875]  eta: 0:00:01  loss: 0.002023 (0.002482)  lr: 0.000062 (0.000062)  wd: 0.057117 (0.055954)  time: 1.397510  data: 0.166769  max mem: 10519\n",
      "Epoch: [13/100] Total time: 0:43:41 (1.398166 s / it)\n",
      "Averaged stats: loss: 0.002023 (0.002482)  lr: 0.000062 (0.000062)  wd: 0.057117 (0.055954)\n",
      "Epoch: [14/100]  [   0/1875]  eta: 0:43:52  loss: 0.002473 (0.002473)  lr: 0.000062 (0.000062)  wd: 0.057131 (0.057131)  time: 1.403757  data: 0.173225  max mem: 10519\n",
      "Epoch: [14/100]  [1000/1875]  eta: 0:20:25  loss: 0.002260 (0.002458)  lr: 0.000062 (0.000062)  wd: 0.058425 (0.057781)  time: 1.403748  data: 0.172534  max mem: 10519\n",
      "Epoch: [14/100]  [1874/1875]  eta: 0:00:01  loss: 0.002003 (0.002290)  lr: 0.000062 (0.000062)  wd: 0.059604 (0.058361)  time: 1.397331  data: 0.166592  max mem: 10519\n",
      "Epoch: [14/100] Total time: 0:43:44 (1.399817 s / it)\n",
      "Averaged stats: loss: 0.002003 (0.002290)  lr: 0.000062 (0.000062)  wd: 0.059604 (0.058361)\n",
      "Epoch: [15/100]  [   0/1875]  eta: 0:43:35  loss: 0.002097 (0.002097)  lr: 0.000062 (0.000062)  wd: 0.059619 (0.059619)  time: 1.394871  data: 0.161922  max mem: 10519\n",
      "Epoch: [15/100]  [1000/1875]  eta: 0:20:24  loss: 0.002205 (0.002361)  lr: 0.000062 (0.000062)  wd: 0.060996 (0.060311)  time: 1.401331  data: 0.169779  max mem: 10519\n",
      "Epoch: [15/100]  [1874/1875]  eta: 0:00:01  loss: 0.002025 (0.002208)  lr: 0.000062 (0.000062)  wd: 0.062249 (0.060928)  time: 1.399793  data: 0.168503  max mem: 10519\n",
      "Epoch: [15/100] Total time: 0:43:43 (1.399367 s / it)\n",
      "Averaged stats: loss: 0.002025 (0.002208)  lr: 0.000062 (0.000062)  wd: 0.062249 (0.060928)\n",
      "Epoch: [16/100]  [   0/1875]  eta: 0:43:25  loss: 0.002008 (0.002008)  lr: 0.000062 (0.000062)  wd: 0.062265 (0.062265)  time: 1.389782  data: 0.159386  max mem: 10519\n",
      "Epoch: [16/100]  [1000/1875]  eta: 0:20:24  loss: 0.002219 (0.002247)  lr: 0.000062 (0.000062)  wd: 0.063725 (0.062999)  time: 1.401789  data: 0.170777  max mem: 10519\n",
      "Epoch: [16/100]  [1874/1875]  eta: 0:00:01  loss: 0.001916 (0.002118)  lr: 0.000062 (0.000062)  wd: 0.065050 (0.063652)  time: 1.400984  data: 0.169936  max mem: 10519\n",
      "Epoch: [16/100] Total time: 0:43:44 (1.399616 s / it)\n",
      "Averaged stats: loss: 0.001916 (0.002118)  lr: 0.000062 (0.000062)  wd: 0.065050 (0.063652)\n",
      "Epoch: [17/100]  [   0/1875]  eta: 0:43:51  loss: 0.001782 (0.001782)  lr: 0.000062 (0.000062)  wd: 0.065066 (0.065066)  time: 1.403386  data: 0.171345  max mem: 10519\n",
      "Epoch: [17/100]  [1000/1875]  eta: 0:20:25  loss: 0.002009 (0.003423)  lr: 0.000061 (0.000062)  wd: 0.066608 (0.065841)  time: 1.399427  data: 0.168255  max mem: 10519\n",
      "Epoch: [17/100]  [1874/1875]  eta: 0:00:01  loss: 0.001920 (0.002745)  lr: 0.000061 (0.000061)  wd: 0.068003 (0.066530)  time: 1.393173  data: 0.162265  max mem: 10519\n",
      "Epoch: [17/100] Total time: 0:43:43 (1.399449 s / it)\n",
      "Averaged stats: loss: 0.001920 (0.002745)  lr: 0.000061 (0.000061)  wd: 0.068003 (0.066530)\n",
      "Epoch: [18/100]  [   0/1875]  eta: 0:43:34  loss: 0.002157 (0.002157)  lr: 0.000061 (0.000061)  wd: 0.068021 (0.068021)  time: 1.394352  data: 0.164473  max mem: 10519\n",
      "Epoch: [18/100]  [1000/1875]  eta: 0:20:23  loss: 0.001971 (0.002125)  lr: 0.000061 (0.000061)  wd: 0.069642 (0.068836)  time: 1.404163  data: 0.173098  max mem: 10519\n",
      "Epoch: [18/100]  [1874/1875]  eta: 0:00:01  loss: 0.001846 (0.002017)  lr: 0.000061 (0.000061)  wd: 0.071107 (0.069560)  time: 1.394603  data: 0.164468  max mem: 10519\n",
      "Epoch: [18/100] Total time: 0:43:42 (1.398654 s / it)\n",
      "Averaged stats: loss: 0.001846 (0.002017)  lr: 0.000061 (0.000061)  wd: 0.071107 (0.069560)\n",
      "Epoch: [19/100]  [   0/1875]  eta: 0:43:26  loss: 0.001839 (0.001839)  lr: 0.000061 (0.000061)  wd: 0.071125 (0.071125)  time: 1.390242  data: 0.160209  max mem: 10519\n",
      "Epoch: [19/100]  [1000/1875]  eta: 0:20:23  loss: 0.001977 (0.002047)  lr: 0.000061 (0.000061)  wd: 0.072824 (0.071980)  time: 1.395568  data: 0.164997  max mem: 10519\n",
      "Epoch: [19/100]  [1874/1875]  eta: 0:00:01  loss: 0.001811 (0.001957)  lr: 0.000061 (0.000061)  wd: 0.074357 (0.072738)  time: 1.397007  data: 0.166582  max mem: 10519\n",
      "Epoch: [19/100] Total time: 0:43:42 (1.398500 s / it)\n",
      "Averaged stats: loss: 0.001811 (0.001957)  lr: 0.000061 (0.000061)  wd: 0.074357 (0.072738)\n",
      "Epoch: [20/100]  [   0/1875]  eta: 0:43:33  loss: 0.001862 (0.001862)  lr: 0.000061 (0.000061)  wd: 0.074377 (0.074377)  time: 1.393847  data: 0.164267  max mem: 10519\n",
      "Epoch: [20/100]  [1000/1875]  eta: 0:20:23  loss: 0.001864 (0.001995)  lr: 0.000060 (0.000061)  wd: 0.076152 (0.075270)  time: 1.400149  data: 0.168974  max mem: 10519\n",
      "Epoch: [20/100]  [1874/1875]  eta: 0:00:01  loss: 0.001744 (0.001915)  lr: 0.000060 (0.000060)  wd: 0.077752 (0.076062)  time: 1.391695  data: 0.161403  max mem: 10519\n",
      "Epoch: [20/100] Total time: 0:43:41 (1.397938 s / it)\n",
      "Averaged stats: loss: 0.001744 (0.001915)  lr: 0.000060 (0.000060)  wd: 0.077752 (0.076062)\n",
      "Epoch: [21/100]  [   0/1875]  eta: 0:44:01  loss: 0.001945 (0.001945)  lr: 0.000060 (0.000060)  wd: 0.077772 (0.077772)  time: 1.408961  data: 0.179190  max mem: 10519\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-b11a15f60e4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m train_dino(data_path,batch_size,lr,weight_decay,weight_decay_end,min_lr,out_dim,tau,total_epochs,warmup_epochs,momentum_teacher,output_dir,saveckp_freq,\n\u001b[0;32m---> 21\u001b[0;31m             clip_grad,freeze_last_layer)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-46-b95ca4e34265>\u001b[0m in \u001b[0;36mtrain_dino\u001b[0;34m(data_path, batch_size, lr, weight_decay, weight_decay_end, min_lr, out_dim, tau, total_epochs, warmup_epochs, momentum_teacher, output_dir, saveckp_freq, clip_grad, freeze_last_layer)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtotal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         train_stats=train_one_epoch(student,teacher,dino_loss,data_loader,optimizer,lr_schedule,wd_schedule,momentum_schedule,epoch,output_dir,\n\u001b[0;32m---> 59\u001b[0;31m                                     total_epochs,clip_grad,freeze_last_layer)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0msave_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'student'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstudent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'teacher'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mteacher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'optimizer'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdino_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-44-9e4bdc8e29ca>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(student, teacher, dino_loss, data_loader, optimizer, lr_schedule, wd_schedule, momentum_schedule, epoch, output_dir, total_epochs, clip_grad, freeze_last_layer)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmetric_logger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMetricLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"  \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Epoch: [{}/{}]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;31m# update weight decay and learning rate according to their schedule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mit\u001b[0m  \u001b[0;31m# global training iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-ccf7888f8819>\u001b[0m in \u001b[0;36mlog_every\u001b[0;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[1;32m    127\u001b[0m             ])\n\u001b[1;32m    128\u001b[0m         \u001b[0mMB\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1024.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m1024.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mdata_time\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-9f3b137093d1>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mall_crops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mall_crops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mall_crops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_local_crops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mall_crops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-35-9f3b137093d1>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mall_crops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mall_crops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mall_crops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_local_crops\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mall_crops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault_float_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_path='data/MNIST - JPG - training'\n",
    "#data_path='../input/mnistasjpg'\n",
    "batch_size=32\n",
    "lr=0.0005\n",
    "weight_decay=0.04\n",
    "weight_decay_end=0.4\n",
    "min_lr=1e-6\n",
    "\n",
    "total_epochs=100\n",
    "warmup_epochs=10\n",
    "momentum_teacher=0.995\n",
    "out_dim=16384\n",
    "tau=0.1\n",
    "saveckp_freq=5\n",
    "\n",
    "clip_grad=3.0\n",
    "freeze_last_layer=1\n",
    "output_dir='log'\n",
    "\n",
    "train_dino(data_path,batch_size,lr,weight_decay,weight_decay_end,min_lr,out_dim,tau,total_epochs,warmup_epochs,momentum_teacher,output_dir,saveckp_freq,\n",
    "            clip_grad,freeze_last_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1C8TnuXRbk_"
   },
   "source": [
    "# visualize attention map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRgXG8AGZMd5"
   },
   "outputs": [],
   "source": [
    "import ipywidgets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device='cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "patch_size=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvjy3_8yZZRW"
   },
   "outputs": [],
   "source": [
    "#models = torch.load(\"/content/drive/MyDrive/MNIST/log/checkpoint10.pth\", map_location=\"cpu\")\n",
    "model=VisionTransformer(patch_size=patch_size)\n",
    "for p in model.parameters():\n",
    "    p.requires_grad=False\n",
    "    \n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "state_dict=torch.load('log/checkpoint20.pth',map_location='cpu')\n",
    "state_dict=state_dict['teacher']\n",
    "# remove `module.` prefix\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "# remove `backbone.` prefix induced by multicrop wrapper\n",
    "state_dict = {k.replace(\"backbone.\", \"\"): v for k, v in state_dict.items()}\n",
    "msg = model.load_state_dict(state_dict, strict=False)\n",
    "dataset = ImageFolder(\"data/MNIST - JPG - testing\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.Resize(112),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,)),\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 592,
     "referenced_widgets": [
      "ff28fabf9f7a474aaf266137b70b6fb5",
      "47145dee9f1843bc9b062c869d98fe75",
      "793aef15122349c8a3cc024ad7eb5ace",
      "793b02bc8e8142a69ed247c2fc49a9bf",
      "cf17a55d180244fd8f27fddd0879814a",
      "6b0063c8af3e4566b5b2efe380815e6a",
      "d838dc193dfe4a30b8b72c80c7dd3b58",
      "802664d17f0b4e67897ccf456172db83",
      "04bed06f45634d408e6927397b61eae7",
      "74be870b88414dbf984696acc1b81959"
     ]
    },
    "executionInfo": {
     "elapsed": 774,
     "status": "ok",
     "timestamp": 1654222690884,
     "user": {
      "displayName": "Yanran Zhang",
      "userId": "00510483234249847034"
     },
     "user_tz": -120
    },
    "id": "LCS_XLKOaOKE",
    "outputId": "7b9bb5e2-5264-4fa1-c53a-71fb04f0aa4c"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff28fabf9f7a474aaf266137b70b6fb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, continuous_update=False, description='i', max=9999), IntSlider(value=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@ipywidgets.interact\n",
    "def _(\n",
    "    i=ipywidgets.IntSlider(min=0, max=len(dataset) - 1, continuous_update=False),\n",
    "    k=ipywidgets.IntSlider(min=0, max=195, value=10, continuous_update=False),\n",
    "    \n",
    "):\n",
    "    img0 = dataset[i][0]\n",
    "    img_ori_tr=transforms.Resize(112)\n",
    "    img_ori=img_ori_tr(img0) #original image\n",
    "    img=transform(img0)\n",
    "        \n",
    "\n",
    "    # make the image divisible by the patch size\n",
    "    w, h = img.shape[1] - img.shape[1] % patch_size, img.shape[2] - img.shape[2] % patch_size\n",
    "    img = img[:, :w, :h].unsqueeze(0) #[1,3,img_size,img_size]\n",
    "    #how many token attention across width & height dim\n",
    "    w_featmap = img.shape[-2] // patch_size \n",
    "    h_featmap = img.shape[-1] // patch_size \n",
    "\n",
    "    #take from the last layer, return the attention coefficents\n",
    "    #dim: [1,num_head,w_featmap*h_featmap+1,w_featmap*h_geatmap+1] \n",
    "    attentions = model.get_last_selfattention(img.to(device))\n",
    "\n",
    "    nh = attentions.shape[1] # number of head\n",
    "\n",
    "    # we keep only the output patch attention map -> [1,take_all_head,take_cls_token,attention coefficent without cross cls_token]\n",
    "    #dim: [num_head,w_featmap*h_featmap]\n",
    "    attentions = attentions[0, :, 0, 1:].reshape(nh, -1)\n",
    "    attentions = attentions.reshape(nh, w_featmap, h_featmap) #[num_head,w_featmap,h_featmap]\n",
    "    attentions = nn.functional.interpolate(attentions.unsqueeze(0), scale_factor=patch_size, mode=\"nearest\")[0].cpu().numpy()\n",
    "    \n",
    "\n",
    "    # original image\n",
    "    plt.imshow(img_ori)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "    #kwargs = {\"vmin\": 0, \"vmax\": 0.24}\n",
    "    # Attentions\n",
    "    n_heads = 7\n",
    "\n",
    "    fig, axs = plt.subplots(2, 3)\n",
    "    \n",
    "    for i in range(n_heads):\n",
    "        #ax = axs[i // 3, i % 3]\n",
    "        #ax.imshow(attentions[i], **kwargs)\n",
    "        ax.imshow(attentions[i])\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "        \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dino.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "04bed06f45634d408e6927397b61eae7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "47145dee9f1843bc9b062c869d98fe75": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": false,
      "description": "i",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_6b0063c8af3e4566b5b2efe380815e6a",
      "max": 9999,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_d838dc193dfe4a30b8b72c80c7dd3b58",
      "value": 0
     }
    },
    "6b0063c8af3e4566b5b2efe380815e6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "74be870b88414dbf984696acc1b81959": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "793aef15122349c8a3cc024ad7eb5ace": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "IntSliderModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "IntSliderModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "IntSliderView",
      "continuous_update": false,
      "description": "k",
      "description_tooltip": null,
      "disabled": false,
      "layout": "IPY_MODEL_802664d17f0b4e67897ccf456172db83",
      "max": 195,
      "min": 0,
      "orientation": "horizontal",
      "readout": true,
      "readout_format": "d",
      "step": 1,
      "style": "IPY_MODEL_04bed06f45634d408e6927397b61eae7",
      "value": 10
     }
    },
    "793b02bc8e8142a69ed247c2fc49a9bf": {
     "model_module": "@jupyter-widgets/output",
     "model_module_version": "1.0.0",
     "model_name": "OutputModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/output",
      "_model_module_version": "1.0.0",
      "_model_name": "OutputModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/output",
      "_view_module_version": "1.0.0",
      "_view_name": "OutputView",
      "layout": "IPY_MODEL_74be870b88414dbf984696acc1b81959",
      "msg_id": "",
      "outputs": [
       {
        "data": {
         "image/png": "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\n",
         "text/plain": "<Figure size 432x288 with 1 Axes>"
        },
        "metadata": {
         "needs_background": "light"
        },
        "output_type": "display_data"
       },
       {
        "data": {
         "image/png": "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\n",
         "text/plain": "<Figure size 432x288 with 6 Axes>"
        },
        "metadata": {
         "needs_background": "light"
        },
        "output_type": "display_data"
       }
      ]
     }
    },
    "802664d17f0b4e67897ccf456172db83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf17a55d180244fd8f27fddd0879814a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d838dc193dfe4a30b8b72c80c7dd3b58": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "SliderStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "SliderStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": "",
      "handle_color": null
     }
    },
    "ff28fabf9f7a474aaf266137b70b6fb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "VBoxModel",
     "state": {
      "_dom_classes": [
       "widget-interact"
      ],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "VBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "VBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47145dee9f1843bc9b062c869d98fe75",
       "IPY_MODEL_793aef15122349c8a3cc024ad7eb5ace",
       "IPY_MODEL_793b02bc8e8142a69ed247c2fc49a9bf"
      ],
      "layout": "IPY_MODEL_cf17a55d180244fd8f27fddd0879814a"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
